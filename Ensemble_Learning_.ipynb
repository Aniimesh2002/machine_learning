{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: What is Ensemble Learning in machine learning? Explain the key idea behind it\n",
        "\n",
        "ANSWER:\n",
        "\n",
        "Ensemble Learning is a machine learning technique where we combine multiple individual models (often called weak learners) to build a stronger and more accurate predictive model.\n",
        "\n",
        "Key Idea Behind Ensemble Learning :-\n",
        "\n",
        "A single model may have limitations (e.g., may underfit or overfit).\n",
        "\n",
        "By combining the predictions of several models, we can reduce errors, improve accuracy, and make the system more robust.\n",
        "\n",
        "The main principle is:\n",
        "\n",
        "“A group of weak learners, when combined, can form a strong learner.”"
      ],
      "metadata": {
        "id": "brni0TmNJbQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer :\n",
        "\n",
        "\n",
        "Bagging and Boosting are both ensemble learning techniques but differ in their approach.\n",
        "Bagging (Bootstrap Aggregating) trains multiple models in parallel on different random subsets of the data and combines their outputs through averaging or majority voting. Its main goal is to reduce variance and avoid overfitting; Random Forest is a common example.\n",
        "\n",
        " Boosting, on the other hand, trains models sequentially, where each new model focuses on correcting the mistakes of the previous ones. It assigns higher weights to misclassified instances and combines models through a weighted approach, mainly reducing bias and improving accuracy. Examples include AdaBoost, Gradient Boosting, and XGBoost.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ctI8j6saJbnK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Answer :\n",
        "\n",
        "Bootstrap sampling is a statistical technique where we create new training datasets by randomly selecting samples from the original dataset with replacement. This means the same data point may appear multiple times in a bootstrap sample, while some points may be left out. In Bagging methods like Random Forest, bootstrap sampling is used to train each individual model (usually decision trees) on a different random subset of the data. This introduces diversity among the models, reduces variance, and prevents overfitting. By aggregating the predictions from these diverse models through averaging (for regression) or majority voting (for classification), Bagging achieves more stable and accurate results than a single model.\n"
      ],
      "metadata": {
        "id": "NXvIaNV4Jb2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Out-of-Bag (OOB) samples are the data points that are not included in a bootstrap sample when building an individual model in Bagging methods like Random Forest. Since bootstrap sampling is done with replacement, on average about 63% of the original data is used to train a model, and the remaining ~37% are left out as OOB samples. These OOB samples act like a built-in validation set for each model. The OOB score is obtained by predicting these left-out samples using the model trained without them and then calculating performance metrics (such as accuracy, error rate, etc.). This provides an unbiased estimate of the model’s performance without needing a separate validation dataset, making Random Forests and similar ensemble methods more efficient in evaluation.\n"
      ],
      "metadata": {
        "id": "9IpifJxGJb5i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "Answer :\n",
        "\n",
        "In a single Decision Tree, feature importance is determined by how much each feature contributes to reducing impurity (such as Gini impurity or entropy) at its splitting nodes. The importance of a feature is essentially the total decrease in impurity brought by that feature across all the splits in the tree. However, a single tree can be unstable and highly sensitive to small changes in the data, so its feature importance may not be very reliable.\n",
        "\n",
        " In contrast, a Random Forest is an ensemble of many decision trees trained on bootstrap samples with random feature selection at each split. Feature importance in a Random Forest is computed by averaging the impurity reduction across all trees, or by measuring the drop in model accuracy when a feature’s values are permuted. This makes Random Forest feature importance more robust, stable, and reliable compared to a single decision tree, since it reflects the consensus of many trees rather than the bias of one.\n"
      ],
      "metadata": {
        "id": "2rzlHdD0Jb8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "\n",
        "'''\n",
        "\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feat_importances = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance\n",
        "feat_importances = feat_importances.sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "# Print Top 5 features\n",
        "print(\"Top 5 Important Features:\")\n",
        "print(feat_importances.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-2R_lEtMQmA",
        "outputId": "9ae96d57-febf-441a-e409-49bc132fc153"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "'''\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train a single Decision Tree\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "acc_dt = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Train a Bagging Classifier with Decision Trees\n",
        "bagging = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),  # updated parameter name\n",
        "    n_estimators=50,  # number of trees\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "acc_bag = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# Print accuracies\n",
        "print(\"Accuracy of Single Decision Tree:\", acc_dt)\n",
        "print(\"Accuracy of Bagging Classifier:\", acc_bag)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOOhcbchMdgR",
        "outputId": "fc0f3147-4ebc-49c1-ad86-19d3f4041140"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 0.9333333333333333\n",
            "Accuracy of Bagging Classifier: 0.9333333333333333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n",
        "'''\n",
        "\n",
        "# Import libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split dataset (70% train, 30% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Define Random Forest Classifier\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [None, 3, 5, 7]\n",
        "}\n",
        "\n",
        "# Setup GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,  # 5-fold cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_rf = grid_search.best_estimator_\n",
        "y_pred = best_rf.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Final Accuracy on Test Set:\", final_accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpfjT4p-M8OG",
        "outputId": "2a757ee3-82ab-4f05-a4bc-2770367ab69f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'n_estimators': 150}\n",
            "Final Accuracy on Test Set: 0.9111111111111111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "'''\n",
        "# Import libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California Housing dataset\n",
        "california = fetch_california_housing()\n",
        "X, y = california.data, california.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train Bagging Regressor with Decision Trees\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "y_pred_bag = bagging_reg.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Print MSE for comparison\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", mse_bag)\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", mse_rf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XUWAw_XNKKj",
        "outputId": "40289bbe-9f49-40b7-b4e2-450f92d2f17f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.25787382250585034\n",
            "Mean Squared Error (Random Forest Regressor): 0.25650512920799395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "ANSWER :\n",
        "\n",
        "\n",
        "\n",
        "Step 1: Choose between Bagging or Boosting**\n",
        "\n",
        "* **Bagging** (e.g., Random Forest) reduces variance by averaging predictions of multiple models trained on bootstrap samples. It is useful if the base model tends to overfit.\n",
        "* **Boosting** (e.g., AdaBoost, Gradient Boosting, XGBoost) reduces bias by sequentially training models that correct previous errors. It is helpful if the base model underfits or if you need very high predictive accuracy.\n",
        "* For **loan default prediction**, boosting is often preferred because correctly identifying defaults (rare events) is critical, and boosting can focus on difficult-to-predict cases.\n",
        "\n",
        "\n",
        "\n",
        "Step 2: Handle Overfitting**\n",
        "\n",
        "* Use **cross-validation** to monitor performance on unseen data.\n",
        "* Apply **regularization techniques** in boosting (like learning rate, max_depth, subsampling).\n",
        "* Limit **tree depth** and **number of estimators** to prevent models from memorizing the training data.\n",
        "* For bagging, ensure **sufficient number of trees** to stabilize predictions without overfitting.\n",
        "\n",
        "\n",
        "\n",
        "Step 3: Select Base Models**\n",
        "\n",
        "* Choose **simple models** as base learners to benefit from ensembling.\n",
        "* Decision Trees are commonly used for both Bagging and Boosting.\n",
        "* In some cases, you could also experiment with logistic regression or small neural networks, depending on feature types.\n",
        "\n",
        "\n",
        "\n",
        "Step 4: Evaluate Performance Using Cross-Validation**\n",
        "\n",
        "* Perform **k-fold cross-validation** (e.g., 5-fold or 10-fold) to evaluate model performance on multiple splits of the data.\n",
        "* Track metrics like:\n",
        "\n",
        "  * **Accuracy** (general performance)\n",
        "  * **Precision & Recall** (important for imbalanced classes, e.g., default vs non-default)\n",
        "  * **ROC-AUC** (overall ability to rank high-risk cases correctly)\n",
        "* Compare ensemble methods (Bagging vs Boosting) using these metrics to select the best approach.\n",
        "\n",
        "\n",
        "\n",
        "Step 5: Justify How Ensemble Learning Improves Decision-Making**\n",
        "\n",
        "* Ensemble models **combine multiple perspectives** to reduce errors and improve reliability.\n",
        "* For loan default prediction:\n",
        "\n",
        "  * Reduces **false negatives** → fewer risky loans are mistakenly approved.\n",
        "  * Reduces **false positives** → fewer good customers are rejected.\n",
        "* Provides a more **robust, stable prediction** than a single model, which is critical in financial decision-making where mistakes can be costly.\n",
        "* Boosting models can also assign **importance to features**, helping the institution understand key risk factors.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "----\n",
        "a sample Python code that demonstrates a complete workflow for loan default prediction using ensemble methods (Boosting and Bagging)\n",
        "including data preprocessing, model training, cross-validation, and evaluation metrics. Since we don’t have a real loan dataset\n",
        "we can use a synthetic dataset using sklearn.make_classification that mimics imbalanced classes like defaults.\n",
        "----\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "# Import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
        "\n",
        "# Step 1: Generate synthetic loan dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=5000,      # number of customers\n",
        "    n_features=20,       # demographic + transaction features\n",
        "    n_informative=10,    # important features\n",
        "    n_redundant=5,       # redundant features\n",
        "    n_clusters_per_class=2,\n",
        "    weights=[0.8, 0.2],  # imbalanced classes: 0=non-default, 1=default\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Train Bagging model (Random Forest)\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=None, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "\n",
        "# Step 4: Train Boosting model (Gradient Boosting)\n",
        "gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gb.fit(X_train, y_train)\n",
        "y_pred_gb = gb.predict(X_test)\n",
        "\n",
        "# Step 5: Evaluate performance\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    print(f\"--- {model_name} ---\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
        "    print(\"Recall:\", recall_score(y_true, y_pred))\n",
        "    print(\"ROC-AUC:\", roc_auc_score(y_true, y_pred))\n",
        "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_true, y_pred))\n",
        "    print(\"\\n\")\n",
        "\n",
        "evaluate_model(y_test, y_pred_rf, \"Random Forest (Bagging)\")\n",
        "evaluate_model(y_test, y_pred_gb, \"Gradient Boosting\")\n",
        "\n",
        "# Step 6: Cross-validation scores\n",
        "cv_scores_rf = cross_val_score(rf, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "cv_scores_gb = cross_val_score(gb, X_train, y_train, cv=5, scoring='roc_auc')\n",
        "\n",
        "print(\"Random Forest CV ROC-AUC mean:\", np.mean(cv_scores_rf))\n",
        "print(\"Gradient Boosting CV ROC-AUC mean:\", np.mean(cv_scores_gb))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36wbjYjTNuMt",
        "outputId": "90a69b65-31fb-486f-8f55-20f9aeb87e78"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Random Forest (Bagging) ---\n",
            "Accuracy: 0.9413333333333334\n",
            "Precision: 0.9821428571428571\n",
            "Recall: 0.7236842105263158\n",
            "ROC-AUC: 0.8601698644604824\n",
            "Confusion Matrix:\n",
            " [[1192    4]\n",
            " [  84  220]]\n",
            "\n",
            "\n",
            "--- Gradient Boosting ---\n",
            "Accuracy: 0.9366666666666666\n",
            "Precision: 0.9523809523809523\n",
            "Recall: 0.7236842105263158\n",
            "ROC-AUC: 0.8572434430558\n",
            "Confusion Matrix:\n",
            " [[1185   11]\n",
            " [  84  220]]\n",
            "\n",
            "\n",
            "Random Forest CV ROC-AUC mean: 0.9671538446714084\n",
            "Gradient Boosting CV ROC-AUC mean: 0.9526103548090046\n"
          ]
        }
      ]
    }
  ]
}