{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMr9C73CQrAv"
      },
      "outputs": [],
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "ANSWER:\n",
        "What is a Decision Tree?\n",
        "A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks.\n",
        "In classification, it is used to assign a class label (e.g., spam or not spam, yes or no, disease A or disease B).\n",
        "It resembles a tree-like structure:\n",
        "Root Node → represents the entire dataset and the first decision to be made.\n",
        "Internal Nodes → represent conditions or tests on features (e.g., \"Is age > 30?\").\n",
        "Branches → outcomes of the decision (Yes/No or True/False).\n",
        "Leaf Nodes → represent the final class labels (e.g., \"Approve loan\" or \"Reject loan\").\n",
        "\n",
        "How does it work in Classification?\n",
        "Start with all data at the root node.\n",
        "Select the best attribute (feature) to split on using a criterion like:\n",
        "Information Gain (based on Entropy)\n",
        "Gini Impurity\n",
        "Chi-Square\n",
        "The goal is to create groups that are as pure (homogeneous) as possible.\n",
        "Split the dataset into subsets based on the chosen feature.\n",
        "Example:\n",
        "Feature: \"Age > 30\" → Yes branch, No branch.\n",
        "Repeat recursively on each subset until:\n",
        "All samples in a node belong to the same class, or\n",
        "No further meaningful splits can be made, or\n",
        "A stopping condition is reached (like maximum depth or minimum samples per leaf).\n",
        "Assign class labels to the leaf nodes based on the majority class of data points in that node."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "ANSWER:\n",
        "\n",
        "\n",
        "Gini Impurity and Entropy are two measures of impurity used in decision trees to determine how good a split is.\n",
        "Both aim to capture how mixed the classes are within a node, since a perfectly pure node is one where all data points belong to the same class.\n",
        "\n",
        "Gini Impurity reflects the probability of misclassifying a randomly chosen element if it were assigned a class label at random according to the class proportions in the node.\n",
        "   A node with Gini equal to zero is completely pure, while higher values indicate greater class mixing. Entropy,\n",
        " on the other hand, comes from information theory and measures the amount of uncertainty or disorder in the class distribution.\n",
        "   A node with low entropy is highly ordered, meaning its samples mostly belong to one class, whereas high entropy indicates more uniform mixing across classes.\n",
        "\n",
        "When building a decision tree, the algorithm evaluates all possible splits and chooses the one that most reduces impurity.\n",
        "If entropy is used, the split that maximizes information gain—that is, the reduction in entropy—is chosen.\n",
        "If Gini is used, the split that minimizes Gini impurity is preferred. Although both criteria usually lead to similar results, there are subtle differences. Gini tends to be more sensitive to the presence of a dominant class and often isolates the majority class earlier. Entropy, by contrast, is more sensitive to how evenly distributed the classes are and therefore encourages splits that produce more balanced partitions.\n",
        "\n",
        "In essence, both impurity measures guide the tree toward purer child nodes, but they differ slightly in how they value class distributions. This difference can influence the structure and depth of the resulting tree, even if the final classification accuracy is often similar.\n",
        "\n"
      ],
      "metadata": {
        "id": "y2pR6lUOQ1FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each.\n",
        "ANSWER:\n",
        "Pre-pruning and post-pruning are two strategies used to prevent decision trees from growing too complex and overfitting the training data,\n",
        "but they differ in when the stopping or simplification happens.\n",
        "\n",
        "In pre-pruning, the tree growth is restricted *during* the construction phase itself.\n",
        "The algorithm decides in advance whether a node should be split further by checking conditions such as the maximum depth,\n",
        " the minimum number of samples required in a node, or whether the impurity reduction from a potential split is large enough.\n",
        " If these conditions are not met, the split is stopped early, and the node is left as a leaf.\n",
        "\n",
        "  A practical advantage of pre-pruning is that it makes the training process more efficient because the algorithm avoids exploring unnecessary\n",
        "  branches of the tree.\n",
        "\n",
        "In post-pruning, on the other hand, the tree is allowed to grow fully until it either perfectly classifies\n",
        "the training data or cannot be split further. Once the complete tree is built, it is then simplified by\n",
        "removing branches or subtrees that do not improve generalization. This pruning step is usually guided by performance\n",
        "on a validation set or by applying statistical tests to check whether further splits add meaningful predictive power.\n",
        "A practical advantage of post-pruning is that it generally produces more accurate and reliable trees because it allows\n",
        "the algorithm to first capture all possible patterns before deciding which parts are noise and should be removed.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kah9rc5QS5DQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "ANSWER:\n",
        "Information gain is a concept from information theory that is used in decision trees to evaluate how good a particular split is.\n",
        "It measures the reduction in uncertainty or impurity about the target class that results from partitioning the data based on a given attribute.\n",
        "In other words, it tells us how much “information” about the class labels we gain by knowing the value of a certain feature.\n",
        "\n",
        "Mathematically, information gain is defined as the difference between the entropy of the parent node and\n",
        "the weighted average entropy of the child nodes after the split. If a split results in child nodes that are much purer than the parent,\n",
        "the information gain will be high; if the classes remain mixed, the gain will be low.\n",
        "\n",
        "The importance of information gain lies in its role as the criterion for selecting the best attribute at each step in building the decision tree.\n",
        "By choosing the split with the highest information gain, the algorithm ensures that each decision reduces uncertainty as much as possible and pushes\n",
        "the data closer to pure class separation. This greedy strategy of maximizing information gain at each node is what guides the tree toward effective\n",
        "classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "sHWT8ZUJS5Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "ANSWER:\n",
        "\n",
        "### **Real-World Applications of Decision Trees**\n",
        "\n",
        "* **Medical Diagnosis** → Classifying patients based on symptoms and test results.\n",
        "* **Finance** → Credit scoring, loan approval, fraud detection.\n",
        "* **Marketing** → Customer segmentation, churn prediction, targeted advertising.\n",
        "* **Business Operations** → Risk assessment and decision-making support.\n",
        "* **Machine Learning** → Basis for ensemble models like Random Forests and Gradient Boosting.\n",
        "\n",
        "---\n",
        "\n",
        "### **Advantages of Decision Trees**\n",
        "\n",
        "* Easy to **understand and interpret**, even for non-technical users.\n",
        "* Requires **little data preprocessing** (no need for scaling or normalization).\n",
        "* Handles both **categorical and numerical features** naturally.\n",
        "* Can capture **non-linear relationships** between features and outcomes.\n",
        "\n",
        "---\n",
        "\n",
        "### **Limitations of Decision Trees**\n",
        "\n",
        "* **Prone to overfitting** if not pruned or regularized.\n",
        "* **Unstable**: small changes in data can drastically change the tree structure.\n",
        "* May have **lower predictive accuracy** compared to advanced models.\n",
        "* Struggles with **high-dimensional datasets** unless used in ensembles.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DkioUJEkS5UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data        # Features\n",
        "y = iris.target      # Target labels\n",
        "\n",
        "# 2. Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Decision Tree Classifier using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 5. Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9QtHyoLS5XV",
        "outputId": "ec4a0b56-d0e5-4db1-f690-9a6420e48b76"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.0\n",
            "\n",
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0167\n",
            "petal length (cm): 0.9061\n",
            "petal width (cm): 0.0772\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "'''\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Decision Tree with max_depth=3\n",
        "tree_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "tree_limited.fit(X_train, y_train)\n",
        "\n",
        "# 4. Train a fully-grown Decision Tree (no max_depth limit)\n",
        "tree_full = DecisionTreeClassifier(random_state=42)\n",
        "tree_full.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions\n",
        "y_pred_limited = tree_limited.predict(X_test)\n",
        "y_pred_full = tree_full.predict(X_test)\n",
        "\n",
        "# 6. Evaluate accuracy\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "print(\"Accuracy of Decision Tree with max_depth=3:\", accuracy_limited)\n",
        "print(\"Accuracy of Fully-grown Decision Tree:\", accuracy_full)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5jOdYsLV1NL",
        "outputId": "31efc0cd-0e7c-407c-993b-3969589662d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Decision Tree with max_depth=3: 1.0\n",
            "Accuracy of Fully-grown Decision Tree: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 8: Write a Python program to:\n",
        "● Load the Boston Housing Dataset\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "'''\n",
        "# Import required libraries\n",
        "#Note: In recent versions of scikit-learn, load_boston()\n",
        "#has been removed because of ethical concerns. We can use fetch_openml(\"boston\", version=1) instead.\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the Boston Housing dataset\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=True)\n",
        "X = boston.data\n",
        "y = boston.target\n",
        "\n",
        "# 2. Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# 5. Compute Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error (MSE):\", mse)\n",
        "\n",
        "# 6. Print feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "for feature_name, importance in zip(X.columns, regressor.feature_importances_):\n",
        "    print(f\"{feature_name}: {importance:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JueHVeinWDlw",
        "outputId": "f26c0fbd-4679-470f-fb85-64b29cab250a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 10.416078431372549\n",
            "\n",
            "Feature Importances:\n",
            "CRIM: 0.0513\n",
            "ZN: 0.0034\n",
            "INDUS: 0.0058\n",
            "CHAS: 0.0000\n",
            "NOX: 0.0271\n",
            "RM: 0.6003\n",
            "AGE: 0.0136\n",
            "DIS: 0.0707\n",
            "RAD: 0.0019\n",
            "TAX: 0.0125\n",
            "PTRATIO: 0.0110\n",
            "B: 0.0090\n",
            "LSTAT: 0.1933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "'''\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define the Decision Tree Classifier\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# 4. Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# 5. Perform GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=dtree,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy'\n",
        ")\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Get best parameters\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "\n",
        "# 7. Evaluate the model with best parameters\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Model Accuracy with Best Parameters:\", accuracy)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg9KX53cXVM1",
        "outputId": "10dce440-3b7a-4905-e266-923218129bed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy with Best Parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "ANSWER:\n",
        "\n",
        "### **1. Handle Missing Values**\n",
        "\n",
        "The first step is to identify and handle missing data.\n",
        "\n",
        "* For **numerical features**, missing values can be imputed using the **mean, median, or a model-based approach**. Median is often preferred for healthcare data to reduce the influence of outliers.\n",
        "* For **categorical features**, missing values can be imputed using the **most frequent category** or a special “Unknown” category.\n",
        "* It is important to perform imputation **after splitting the data** into training and testing sets to avoid data leakage.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Encode Categorical Features**\n",
        "\n",
        "Decision Trees can handle categorical features in some implementations, but many libraries require numeric encoding:\n",
        "\n",
        "* **Label Encoding**: Convert categories into integer labels (useful if ordinal relationships exist).\n",
        "* **One-Hot Encoding**: Convert categories into binary dummy variables (common for non-ordinal categories).\n",
        "\n",
        "Encoding ensures the tree can process the data correctly while preserving information.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Train a Decision Tree Model**\n",
        "\n",
        "* Select a **Decision Tree Classifier** because the target is categorical (disease present/absent).\n",
        "* Split the dataset into **training and testing sets**.\n",
        "* Fit the tree on the training data.\n",
        "* Start with default hyperparameters initially to get a baseline model.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Tune Hyperparameters**\n",
        "\n",
        "To improve performance and prevent overfitting:\n",
        "\n",
        "* Use **GridSearchCV** or **RandomizedSearchCV** for cross-validated hyperparameter tuning.\n",
        "* Important parameters include:\n",
        "\n",
        "  * `max_depth`: Limits tree depth to avoid overfitting.\n",
        "  * `min_samples_split`: Minimum samples required to split a node.\n",
        "  * `min_samples_leaf`: Minimum samples required in a leaf node.\n",
        "  * `criterion`: Gini impurity or entropy.\n",
        "* Select hyperparameters that **maximize cross-validated accuracy or F1-score**.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Evaluate Model Performance**\n",
        "\n",
        "* Evaluate using the **test set** to measure generalization.\n",
        "* Metrics to consider:\n",
        "\n",
        "  * **Accuracy**: Overall correctness.\n",
        "  * **Precision and Recall**: Especially important in healthcare to avoid false negatives.\n",
        "  * **F1-score**: Balances precision and recall.\n",
        "  * **ROC-AUC**: Measures ability to discriminate between classes.\n",
        "* Optionally, inspect **feature importances** to understand which factors contribute most to disease prediction.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Business Value of the Model**\n",
        "\n",
        "* **Early Detection**: Identifies patients at high risk before symptoms worsen, enabling timely intervention.\n",
        "* **Resource Allocation**: Helps hospitals prioritize testing and treatment for high-risk patients.\n",
        "* **Cost Reduction**: Reduces unnecessary tests for low-risk patients.\n",
        "* **Decision Support**: Provides actionable insights to doctors, improving treatment planning.\n",
        "* **Public Health Insights**: Aggregated predictions can guide preventive care and policy decisions.\n",
        "\n",
        "---\n",
        " Here’s a complete Python workflow for your healthcare disease prediction scenario using a Decision Tree.\n",
        " This example covers missing value handling, encoding categorical features, training, hyperparameter tuning, and evaluation.\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# --- 1. Load dataset (example CSV) ---\n",
        "# Replace 'healthcare_data.csv' with your actual dataset\n",
        "data = pd.read_csv('healthcare_data.csv')\n",
        "\n",
        "# Assume 'target' is the disease column (1 = disease, 0 = no disease)\n",
        "X = data.drop('target', axis=1)\n",
        "y = data['target']\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = X.select_dtypes(include=['object', 'category']).columns\n",
        "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# --- 2. Preprocessing: Handle missing values and encode categorical features ---\n",
        "numerical_transformer = SimpleImputer(strategy='median')  # Impute numerical missing values with median\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing categorical values\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))      # One-hot encode categories\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numerical_transformer, numerical_cols),\n",
        "    ('cat', categorical_transformer, categorical_cols)\n",
        "])\n",
        "\n",
        "# --- 3. Split into training and testing sets ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- 4. Create pipeline with Decision Tree ---\n",
        "dtree = DecisionTreeClassifier(random_state=42)\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('classifier', dtree)])\n",
        "\n",
        "# --- 5. Hyperparameter tuning using GridSearchCV ---\n",
        "param_grid = {\n",
        "    'classifier__max_depth': [3, 5, 7, None],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4],\n",
        "    'classifier__criterion': ['gini', 'entropy']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# --- 6. Best parameters and model ---\n",
        "best_params = grid_search.best_params_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# --- 7. Evaluate the model ---\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(\"\\nEvaluation Metrics:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1-Score:\", f1_score(y_test, y_pred))\n",
        "# ROC-AUC for binary classification\n",
        "y_pred_prob = best_model.predict_proba(X_test)[:,1]\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_prob))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "q-0xXx4tXu9E",
        "outputId": "403e30ab-0784-42c4-c7d1-dd042c7dcf32"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'healthcare_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1193711483.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m# --- 1. Load dataset (example CSV) ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m# Replace 'healthcare_data.csv' with your actual dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'healthcare_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;31m# Assume 'target' is the disease column (1 = disease, 0 = no disease)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'healthcare_data.csv'"
          ]
        }
      ]
    }
  ]
}